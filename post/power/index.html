<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="&mldr;Or, can we trust these estimates? One of the most common questions that comes up in scientific publications is &ldquo;Can I trust these conclusions?&rdquo; Statistics is supposed to help us decide the weight of evidence for or against a certain conclusion, but there have been issues. We are realizing that the standard paradim is not holding up, and there are many more false positives in the literature than the p < 0."><meta name=theme-color content="#ffcd00"><meta property="og:title" content="Retrospective Power Analysis • Ten thousand words..."><meta property="og:description" content="&mldr;Or, can we trust these estimates? One of the most common questions that comes up in scientific publications is &ldquo;Can I trust these conclusions?&rdquo; Statistics is supposed to help us decide the weight of evidence for or against a certain conclusion, but there have been issues. We are realizing that the standard paradim is not holding up, and there are many more false positives in the literature than the p < 0."><meta property="og:url" content="https://mespe.github.io/post/power/"><meta property="og:site_name" content="Ten thousand words..."><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:tag" content="R"><meta property="article:tag" content="agriculture"><meta property="article:published_time" content="2018-06-27T16:16:50-06:00"><meta property="article:modified_time" content="2018-06-27T16:16:50-06:00"><meta name=twitter:card content="summary"><meta name=generator content="Hugo 0.101.0"><title>Retrospective Power Analysis • Ten thousand words...</title><link rel=canonical href=https://mespe.github.io/post/power/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/assets/css/main.ab98e12b.css><link rel=stylesheet href=/css/custom.css><style>:root{--color-accent:#ffcd00}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-101069222-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class='page type-post has-sidebar'><div class=site><div id=sidebar class=sidebar><a class=screen-reader-text href=#main-menu>Skip to Main Menu</a><div class=container><section class='widget widget-about sep-after'><header><h2 class='title site-title'><a href=/>Ten thousand words...</a></h2><div class=desc>Thoughts on agriculture, data science, and academia</div></header></section><section class='widget widget-taxonomy_cloud sep-after'><header><h4 class='title widget-title'>Tags</h4></header><div class='container list-container'><ul class='list taxonomy-cloud'><li><a href=/tags/agriculture/ style=font-size:1.25em>agriculture</a></li><li><a href=/tags/blogging/ style=font-size:1em>blogging</a></li><li><a href=/tags/computers/ style=font-size:1.4166666666666665em>computers</a></li><li><a href=/tags/data-science/ style=font-size:1.9166666666666665em>data science</a></li><li><a href=/tags/golang/ style=font-size:1em>Golang</a></li><li><a href=/tags/health/ style=font-size:1.0833333333333333em>health</a></li><li><a href=/tags/hugo/ style=font-size:1em>hugo</a></li><li><a href=/tags/personal/ style=font-size:1.1666666666666667em>personal</a></li><li><a href=/tags/r/ style=font-size:2em>R</a></li><li><a href=/tags/research/ style=font-size:1.0833333333333333em>research</a></li><li><a href=/tags/stan/ style=font-size:1em>Stan</a></li><li><a href=/tags/stats/ style=font-size:1.0833333333333333em>stats</a></li></ul></div></section></div><div class=sidebar-overlay></div></div><div class=main><nav id=main-menu class='menu main-menu' aria-label='Main Menu'><div class=container><a class=screen-reader-text href=#content>Skip to Content</a>
<button id=sidebar-toggler class=sidebar-toggler aria-controls=sidebar>
<span class=screen-reader-text>Toggle Sidebar</span>
<span class=open><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></span><span class=close><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></span></button><ul><li class=item><a href=/>Home</a></li><li class=item><a href=/post/>Blog</a></li><li class=item><a href=/about/>About me</a></li><li class=item><a href=/research/>Research</a></li><li class=item><a href=/consulting/>Consulting</a></li><li class=item><a href=https://github.com/mespe>Repos</a></li><li class=item><a href=/docs/espe_cv.pdf>CV</a></li></ul></div></nav><div class=header-widgets><div class=container></div></div><header id=header class='header site-header'><div class='container sep-after'><div class=header-info><p class='site-title title'>Ten thousand words...</p><p class='desc site-desc'>Thoughts on agriculture, data science, and academia</p></div></div></header><main id=content><article lang=en class=entry><header class='header entry-header'><div class='container sep-after'><div class=header-info><h1 class=title>Retrospective Power Analysis</h1></div><div class=entry-meta><span class=posted-on><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg><span class=screen-reader-text>Posted on</span>
<time class=entry-date datetime=2018-06-27T16:16:50-06:00>2018, Jun 27</time></span>
<span class=reading-time><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 15 15"/></svg>8 mins read</span></div></div></header><details class='container entry-toc'><summary class=title><span>Table of Contents</span></summary><nav id=TableOfContents><ul><li><a href=#example-typical-agricultural-experiment>Example: Typical agricultural experiment</a><ul><li><a href=#defining-the-data-generative-process>Defining the data generative process</a></li></ul></li><li><a href=#what-effect-size-can-they-reliably-detect>What effect size can they reliably detect?</a></li></ul></nav></details><div class='container entry-content'><h1 id=or-can-we-trust-these-estimates>&mldr;Or, can we trust these estimates?</h1><p>One of the most common questions that comes up in scientific publications is &ldquo;Can I trust these conclusions?&rdquo;
Statistics is supposed to help us decide the weight of evidence for or against a certain conclusion, but there have been issues.
We are realizing that the standard paradim is not holding up, and there are many more false positives in the literature than the p &lt; 0.05 cutoff would have us believe.
A large issue is that under-powered studies can result in estimates which are too large (magnitude or Type M errors) or are the opposite sign (sign, or Type S errors).
Or, when we are interested in the difference between two treatments, an underpowered study will no be able to detect the effect.</p><p>One way to resolve these issues and determine how much trust you should put into a piece of work is to conduct a retroactive power analysis.</p><h1 id=what-is-power>What is &ldquo;power&rdquo;?</h1><p>Power is a measure of how likely we are to find an effect.
Our chances of detecting an effect is affected by many factors:</p><ul><li>The effect size: Larger effects are easier to find</li><li>The variation between samples (i.e., the error): Less noise or variation makes effects easier to detect</li><li>Sample size: the more observations we have, the more certain we are of the estimate</li></ul><h1 id=why-conduct-a-power-analysis-after-the-fact>Why conduct a power analysis after the fact?</h1><p>There are many good reasons.
A power analysis can tell you how large of an effect the study could reliably detect.
It is too common for p > 0.05 to be interpretted as &ldquo;no difference between treatments&rdquo; or &ldquo;no effect&rdquo;, but many of those studies were under-powered and never had a chance to detect the effect.
On the other side of the coin, when one finds a significant effect in an underpowered study, odd are that either the sign of the estimate or the magnitude is wrong (or even both).</p><h2 id=example-typical-agricultural-experiment>Example: Typical agricultural experiment</h2><p>For this example, I am going to use this paper by <a href=https://www.sciencedirect.com/science/article/pii/S0378429017319287>Carrijo et al., 2018</a>.
I am using this paper because I am familiar with the system, have auxiliary data to leverage, and it is open access so anyone can follow along.
To be clear, this is just one example - similar examples can be found throughout the scientific literature.</p><p>That said, this paper is prime example of where a lack of statistical significance was interpretted as &ldquo;no difference&rdquo; rather than the more accurate &ldquo;the difference was smaller than our power to detect&rdquo;. So, what size of effect size could they have detected?</p><p><em>Note: although there are packages which can do this more or less automatically, we will do it &ldquo;by hand&rdquo; for demonstration purposes.</em></p><p>First, we need to know the experimental design. The authors report a 2 year experiment with a randomized complete block design with 3 replications and 3 treatments in 2015 and 4 in 2016.</p><p>Now, how were the data analyzed? The authors report a within year (i.e., each year was modeled separately) linear model with block and treatment as fixed effects.</p><p><em>If you just did a double take, then good on you. There are already issues in the analysis.
We will look at the original analysis and as well as a more robust one in a later post.</em></p><p>The last pieces we need are some estimate of magnitude of variation within treatments. We will use a rough estimate based on the numbers as reported by the authors to start.
Specifically, we are going to assume the variation between treatments is shared.</p><p>Now, we don&rsquo;t have access to the original data, so we are going to have to do some estimating for the standard deviation within treatments.
To do this, we are going to change the the standard deviation and will see how different levels of variation affect our power.</p><p>From a previous meta-analysis by these same authors, we expect a roughly 5&ndash;10% effect size.
Taking yield as an example, based on the tables in the paper, we can estimate that the average yield to be around 11 Mg/ha, which makes the <em>a priori</em> estimate of the effect 0.55 &ndash; 1.1.
That is, we expect the AWD treatment to decrease yields by 0.55&ndash;1.1 Mg/ha</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>intercept <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>11</span>
</span></span><span style=display:flex><span>beta_treatment <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>c</span>(<span style=color:#f99b15>0</span>, <span style=color:#f99b15>-0.55</span>, <span style=color:#f99b15>-1.1</span>)
</span></span></code></pre></div><p>Getting the error is a little more difficult. The standard errors reported range from 0.08 to 0.48.
But this will include both block to variation as well as the within treatment variation.
We will work with the optimistic assumption that the blocks have a low variance.</p><p>Since we are not really interested in the blocks, we are just going to set them with roughly a standard deviation of 0.075 and arrange them so that their effects sum to zero,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>beta_block <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>c</span>(<span style=color:#f99b15>0.15</span>, <span style=color:#f99b15>0</span>, <span style=color:#f99b15>-0.15</span>)
</span></span></code></pre></div><p>As we will see, the model assumes a shared error term for all observations.
In other studies working at this same site, I have estimated the measurement error to be roughly 0.9 Mg/ha. If we are optimistic that the standard error is lower here, we
can get an estimate of the measurement error from the standard errors reported (standard error = \( \sigma / \sqrt{n} \) ).
Based on the numbers reported in the paper and this equation,
we will use 0.34 Mg/ha (very optimistic based on other data from this site).</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>sigma <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>0.34</span>
</span></span></code></pre></div><p>Now, we can set up our simulation framework.</p><h3 id=defining-the-data-generative-process>Defining the data generative process</h3><p>Ok, now that we have set the of the experiment variables, we need to make a function that defines the data generative model.
What does that mean?
Well, our statistical analysis assumes the data are being generated in a certain way.
We want to mimic that in order to generate some fake data.</p><p>Since the authors state they used a simple linear model, we know the mean variable is assumed to be a linear function of the form:</p><p>$$ y_{ij} = intercept + \beta_{treatment\ i} + \beta_{block\ j} + \epsilon $$</p><p>We need to convert this into a function which produces random data when called, and takes the parameters as inputs. Since we want this to produce <strong>random</strong> data, we have two choices:</p><ol><li>we can provide a fixed set of betas, and then the only randomness is from the error term,</li><li>we can allow the betas to vary within a range</li></ol><p>Since the first generally aligns with the frequentist interpretation of parameters, we will use that one.
We will use the fixed estimates for beta defined earlier.</p><p>Now we will define a function to simulate data,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>simulate_data <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>function</span>(intercept,
</span></span><span style=display:flex><span>                         beta_treatments,
</span></span><span style=display:flex><span>                         beta_blocks,
</span></span><span style=display:flex><span>                         sigma)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    n_treat <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>length</span>(beta_treatments)
</span></span><span style=display:flex><span>    n_block <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>length</span>(beta_blocks)
</span></span><span style=display:flex><span>    data_matrix <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>expand.grid</span>(treatment <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>seq</span>(n_treat),
</span></span><span style=display:flex><span>                              block <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>seq</span>(n_block))
</span></span><span style=display:flex><span>    data_matrix<span style=color:#5bc4bf>$</span>mu <span style=color:#5bc4bf>=</span> intercept <span style=color:#5bc4bf>+</span>
</span></span><span style=display:flex><span>        beta_treatments[data_matrix<span style=color:#5bc4bf>$</span>treatment] <span style=color:#5bc4bf>+</span>
</span></span><span style=display:flex><span>        beta_blocks[data_matrix<span style=color:#5bc4bf>$</span>block]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    data_matrix<span style=color:#5bc4bf>$</span>y <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>rnorm</span>(<span style=color:#06b6ef>nrow</span>(data_matrix), data_matrix<span style=color:#5bc4bf>$</span>mu, sigma)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#06b6ef>return</span>(data_matrix)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Next, we want to generate a bunch of random datasets and figure out how often we are able to detect a significant effect.</p><p>Again, we are going to make a function to do this,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>power_analysis <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>function</span>(intercept,
</span></span><span style=display:flex><span>                          beta_treatments,
</span></span><span style=display:flex><span>                          beta_blocks,
</span></span><span style=display:flex><span>                          sigma,
</span></span><span style=display:flex><span>                          alpha <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>0.05</span>)
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    d <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>simulate_data</span>(intercept,
</span></span><span style=display:flex><span>                      beta_treatments,
</span></span><span style=display:flex><span>                      beta_blocks,
</span></span><span style=display:flex><span>                      sigma)
</span></span><span style=display:flex><span>    mod <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>aov</span>(y <span style=color:#5bc4bf>~</span> <span style=color:#06b6ef>factor</span>(treatment) <span style=color:#5bc4bf>+</span> <span style=color:#06b6ef>factor</span>(block), data <span style=color:#5bc4bf>=</span> d)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#776e71># p-value for treatment - logical</span>
</span></span><span style=display:flex><span>    <span style=color:#06b6ef>summary</span>(mod)[[1]]<span style=color:#5bc4bf>$</span><span style=color:#06b6ef>`Pr</span>(<span style=color:#5bc4bf>&gt;</span>F)`[1] <span style=color:#5bc4bf>&lt;</span> alpha
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>and then we will replicate that a bunch of times, say 1000 using the values we established earlier,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#06b6ef>table</span>(<span style=color:#06b6ef>replicate</span>(<span style=color:#f99b15>1000</span>,
</span></span><span style=display:flex><span>                <span style=color:#06b6ef>power_analysis</span>(intercept,
</span></span><span style=display:flex><span>                               beta_treatment,
</span></span><span style=display:flex><span>                               beta_block, sigma)))
</span></span></code></pre></div><pre tabindex=0><code>## 
## FALSE  TRUE 
##   331   669
</code></pre><p>You can see, this study has much lower then the standard 80% power, even with very optimistic assumptions about the measurement error. What if we use a still optimistic but more realistic measurement error of 0.5 Mg/ha?</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#06b6ef>table</span>(<span style=color:#06b6ef>replicate</span>(<span style=color:#f99b15>1000</span>,
</span></span><span style=display:flex><span>                <span style=color:#06b6ef>power_analysis</span>(intercept,
</span></span><span style=display:flex><span>                               beta_treatment,
</span></span><span style=display:flex><span>                               beta_block, sigma <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>0.5</span>)))
</span></span></code></pre></div><pre tabindex=0><code>## 
## FALSE  TRUE 
##   641   359
</code></pre><p>It becomes appearant that this study did not have good chances of detecting an effect in this system. Hence, the conclusion of &ldquo;no difference&rdquo; is suspect.</p><p>Put another way, the correct conclusion from these authors&rsquo; analysis is that the effect, if any, is smaller than their power to detect.</p><h2 id=what-effect-size-can-they-reliably-detect>What effect size can they reliably detect?</h2><p>The natural next question is how large of an effect can this study reliably detect?</p><p>We can answer this using the same framework above.
Again, we will assume a optimistic 0.5 Mg/ha error - approximately half of the average error for experiments at this site.</p><p>We will test a range of effect sizes ranging from -0.25 to -3.5 Mg/ha,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>beta_base <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>c</span>(<span style=color:#f99b15>0</span>, <span style=color:#f99b15>0.5</span>, <span style=color:#f99b15>1</span>)
</span></span><span style=display:flex><span>beta_range <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>seq</span>(<span style=color:#f99b15>0.25</span>, <span style=color:#f99b15>3.5</span>, by <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>0.25</span>) <span style=color:#5bc4bf>*</span> <span style=color:#f99b15>-1</span>
</span></span><span style=display:flex><span>test_cases <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>lapply</span>(beta_range, `*`, beta_base)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ans <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>lapply</span>(test_cases, <span style=color:#06b6ef>function</span>(x)
</span></span><span style=display:flex><span>    <span style=color:#06b6ef>table</span>(<span style=color:#06b6ef>replicate</span>(<span style=color:#f99b15>1000</span>,
</span></span><span style=display:flex><span>                    <span style=color:#06b6ef>power_analysis</span>(intercept,
</span></span><span style=display:flex><span>                                   beta_treatment <span style=color:#5bc4bf>=</span> x,
</span></span><span style=display:flex><span>                                   beta_block, sigma <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>0.5</span>))))
</span></span><span style=display:flex><span>ans <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>as.data.frame</span>(<span style=color:#06b6ef>do.call</span>(rbind, ans)<span style=color:#5bc4bf>/</span><span style=color:#f99b15>10</span>)
</span></span><span style=display:flex><span><span style=color:#06b6ef>colnames</span>(ans) <span style=color:#5bc4bf>=</span> <span style=color:#06b6ef>c</span>(<span style=color:#48b685>&#34;No effect&#34;</span>, <span style=color:#48b685>&#34;Effect&#34;</span>)
</span></span></code></pre></div><p>and then we can plot the results,</p><div class=highlight><pre tabindex=0 style=color:#e7e9db;background-color:#2f1e2e;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#06b6ef>plot</span>(ans<span style=color:#5bc4bf>$</span>Effect <span style=color:#5bc4bf>~</span> beta_range, type <span style=color:#5bc4bf>=</span> <span style=color:#48b685>&#34;b&#34;</span>,
</span></span><span style=display:flex><span>     xlab <span style=color:#5bc4bf>=</span> <span style=color:#48b685>&#34;Effect size (Mg/ha)&#34;</span>,
</span></span><span style=display:flex><span>     ylab <span style=color:#5bc4bf>=</span> <span style=color:#48b685>&#34;Power (%)&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#06b6ef>abline</span>(h <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>80</span>, lty <span style=color:#5bc4bf>=</span> <span style=color:#f99b15>2</span>)
</span></span></code></pre></div><p><img src=/img/unnamed-chunk-9-1.png alt="The estimated power given a range of effect sizes."></p><p>We can see that for this experiment to detect an effect with 80% power, the effect has to be approximately -2.0 Mg/ha, or 18% of yield &ndash; 3 times the estimated effect size from these authors&rsquo; previous work.</p><p>Now, a >10% drop in yield is sufficient to hurt a farmer&rsquo;s bottom line, but here the authors conclude no effect.
Knowing that there could have been up to a 18% decrease in yield and this study would not have been likely to detect it, we should recommend caution applying these treatments.</p><h1 id=conclusions>Conclusions</h1><p>Here, with some straightforward R code, we have seen how retroactive power analysis can help us evaluate the support behind conclusions made in a scientific study.
In this case, it seems unlikely that the study could detect the effect of interest.</p><p><em>Please note, as I said previously, I am not picking on this work or these authors.
These issues exist in many published papers.
This paper just happens to be a good example.</em></p></div><footer class=entry-footer><div class='container sep-before'><div class=categories><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M22 19a2 2 0 01-2 2H4a2 2 0 01-2-2V5A2 2 0 014 3H9l2 3h9a2 2 0 012 2z"/></svg><span class=screen-reader-text>Categories: </span><a class=category href=/categories/r/>R</a>, <a class=category href=/categories/stats/>Stats</a>, <a class=category href=/categories/agriculture/>agriculture</a></div><div class=tags><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2H12l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=screen-reader-text>Tags: </span><a class=tag href=/tags/r/>R</a>, <a class=tag href=/tags/agriculture/>agriculture</a></div></div></footer></article><nav class=entry-nav><div class=container><div class='prev-entry sep-before'><a href=/post/dyn_docs-pt2/><span aria-hidden=true><svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="20" y1="12" x2="4" y2="12"/><polyline points="10 18 4 12 10 6"/></svg>Previous</span>
<span class=screen-reader-text>Previous post: </span>Dynamic documents (part 2)</a></div><div class='next-entry sep-before'><a href=/post/r_best_practice/><span class=screen-reader-text>Next post: </span>Good practices for working in R<span aria-hidden=true>Next<svg class="icon" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><line x1="4" y1="12" x2="20" y2="12"/><polyline points="14 6 20 12 14 18"/></svg></span></a></div></div></nav></main><footer id=footer class=footer><div class='container sep-before'><div class=copyright><p>&copy; 2017-2022 Matt Espe</p></div></div></footer></div></div><script>window.__assets_js_src="/assets/js/"</script><script src=/assets/js/main.c3bcf2df.js></script><script src=/js/custom.js></script></body></html>